[
  {
    "path": "posts/2022-07-12-configure-gpg-and-ssh-for-wsl-on-windows/",
    "title": "Configure GPG and SSH for WSL on Windows",
    "description": "Learn how to properly manage and conigure your precious SSH and GPG keys with Windows Subsystem for Linux.",
    "author": [
      {
        "name": "Jimmy Briggs",
        "url": "https://github.com/jimbrig"
      }
    ],
    "date": "2022-07-12",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nOverview\r\nPre-Requisites\r\nReference: GPG Commands\r\nGenerating\r\nKeys\r\nListing Keys\r\nExporting\r\nKeys\r\nAdd Keys\r\nto GitHub\r\nImporting\r\nKeys\r\nTesting Keys\r\n\r\nScripts\r\nInstallations\r\nBackup GPG\r\nKeys\r\nImport Key\r\nScripts\r\n\r\n\r\nOverview\r\nSee a detailed guide in my core dotfiles repo here: https://github.com/jimbrig/jimsdots/tree/main/gpg.\r\nThis post walks through my routine for setting up and configuring my\r\nWSL distros with SSH and GPG keys.\r\nPre-Requisites\r\nTo follow along you will need the following:\r\nWindows Operating System\r\nWindows Sub System for Linux Installed\r\nA Linux Distribution (i.e. Ubuntu) Installed\r\nGit installed on Windows\r\nWindows %PATH% pointing to the\r\n/usr/bin directory of the Git for Windows Installation\r\nDirectory.\r\nGit and GPG installed in Linux Distribution.\r\nReference: GPG Commands\r\nFirst, let’s cover some basic gpg commands to\r\nfamiliarize ourselves with the command line utility:\r\nRunning gpg --help outputs quite a handful:\r\n> gpg --help\r\n\r\ngpg (GnuPG) 2.2.29-unknown\r\nlibgcrypt 1.9.3-unknown\r\nCopyright (C) 2021 Free Software Foundation, Inc.\r\nLicense GNU GPL-3.0-or-later <https://gnu.org/licenses/gpl.html>\r\nThis is free software: you are free to change and redistribute it.\r\nThere is NO WARRANTY, to the extent permitted by law.\r\n\r\nHome: /c/Users/jimmy/.gnupg\r\nSupported algorithms:\r\nPubkey: RSA, ELG, DSA, ECDH, ECDSA, EDDSA\r\nCipher: IDEA, 3DES, CAST5, BLOWFISH, AES, AES192, AES256, TWOFISH,\r\n        CAMELLIA128, CAMELLIA192, CAMELLIA256\r\nHash: SHA1, RIPEMD160, SHA256, SHA384, SHA512, SHA224\r\nCompression: Uncompressed, ZIP, ZLIB, BZIP2\r\n\r\nSyntax: gpg [options] [files]\r\nSign, check, encrypt or decrypt\r\nDefault operation depends on the input data\r\n\r\nCommands:\r\n\r\n -s, --sign                  make a signature\r\n     --clear-sign            make a clear text signature\r\n -b, --detach-sign           make a detached signature\r\n -e, --encrypt               encrypt data\r\n -c, --symmetric             encryption only with symmetric cipher\r\n -d, --decrypt               decrypt data (default)\r\n     --verify                verify a signature\r\n -k, --list-keys             list keys\r\n     --list-signatures       list keys and signatures\r\n     --check-signatures      list and check key signatures\r\n     --fingerprint           list keys and fingerprints\r\n -K, --list-secret-keys      list secret keys\r\n     --generate-key          generate a new key pair\r\n     --quick-generate-key    quickly generate a new key pair\r\n     --quick-add-uid         quickly add a new user-id\r\n     --quick-revoke-uid      quickly revoke a user-id\r\n     --quick-set-expire      quickly set a new expiration date\r\n     --full-generate-key     full featured key pair generation\r\n     --generate-revocation   generate a revocation certificate\r\n     --delete-keys           remove keys from the public keyring\r\n     --delete-secret-keys    remove keys from the secret keyring\r\n     --quick-sign-key        quickly sign a key\r\n     --quick-lsign-key       quickly sign a key locally\r\n     --quick-revoke-sig      quickly revoke a key signature\r\n     --sign-key              sign a key\r\n     --lsign-key             sign a key locally\r\n     --edit-key              sign or edit a key\r\n     --change-passphrase     change a passphrase\r\n     --export                export keys\r\n     --send-keys             export keys to a keyserver\r\n     --receive-keys          import keys from a keyserver\r\n     --search-keys           search for keys on a keyserver\r\n     --refresh-keys          update all keys from a keyserver\r\n     --import                import/merge keys\r\n     --card-status           print the card status\r\n     --edit-card             change data on a card\r\n     --change-pin            change a card's PIN\r\n     --update-trustdb        update the trust database\r\n     --print-md              print message digests\r\n     --server                run in server mode\r\n     --tofu-policy VALUE     set the TOFU policy for a key\r\n\r\nOptions:\r\n\r\n -a, --armor                 create ascii armored output\r\n -r, --recipient USER-ID     encrypt for USER-ID\r\n -u, --local-user USER-ID    use USER-ID to sign or decrypt\r\n -z N                        set compress level to N (0 disables)\r\n     --textmode              use canonical text mode\r\n -o, --output FILE           write output to FILE\r\n -v, --verbose               verbose\r\n -n, --dry-run               do not make any changes\r\n -i, --interactive           prompt before overwriting\r\n     --openpgp               use strict OpenPGP behavior\r\n\r\n(See the man page for a complete listing of all commands and options)\r\n\r\nExamples:\r\n\r\n -se -r Bob [file]          sign and encrypt for user Bob\r\n --clear-sign [file]        make a clear text signature\r\n --detach-sign [file]       make a detached signature\r\n --list-keys [names]        show keys\r\n --fingerprint [names]      show fingerprints\r\n\r\nPlease report bugs to <https://bugs.gnupg.org>.\r\n\r\nTo skip all the excess hoop-lah, let’s focus on the commands we\r\nneed.\r\nGenerating Keys\r\ngpg --full-generate-key\r\n\r\ngpg --default-new-key-algo rsa4096 --gen-key\r\ngpg --full-generate-key: will generate a GPG key for\r\nyou (select RSA, 4096 Bits, No Expiration,\r\netc.); alternatively you can run\r\ngpg --default-new-key-algo rsa4096 --gen-key\r\n\r\nNote: When asked to enter your email address, ensure that you enter\r\nthe verified email address for your GitHub account. To keep your email\r\naddress private, use your GitHub-provided no-reply email address. For\r\nmore information, see “Verifying your email address” and “Setting your\r\ncommit email address.”\r\n\r\nListing Keys\r\n\r\ngpg --list-secret-keys --keyid-format LONG\r\n\r\ngpg --armor --export <keyid> | clip.exe\r\nNext, list the keys via:\r\ngpg --list-secret-keys --keyid-format LONG and copy the ID\r\nof the key you want to use.\r\ngpg --list-secret-keys: will list your secret key\r\nsignatures\r\ngpg --armor --list-secret-keys: will list your\r\nsecret key signatures using the armor flag which allows for\r\noutput in normal format text.\r\n*NOTE: you can use the shorthand -a flag instead of\r\n--armor also.\r\n\r\nRun gpg --armor --export <keyid> | clip.exe to\r\noutput the key’s text to your clipboard.\r\nExporting Keys\r\nTo export a key to a file run the same command but instead of\r\npiping into the clipboard, utilize the output\r\nredirection > syntax:\r\ngpg -a --export\r\n\r\ngpg --armor --export <keyid> > public.key\r\n\r\ngpg -a --export > publickeys.asc\r\n\r\ngpg -a --export-secret-keys > privatekeys.asc\r\n\r\ngpg --export-secret-keys {{KEY_ID}} > privatekey.key\r\n\r\ngpg --export-ownertrust > ownertrust.txt\r\ngpg -a --export: will export public keys\r\ngpg -a --export > publickeys.asc will export the\r\npublic keys to an importable file.\r\ngpg -a --export-secret-keys > privatekeys.asc will\r\nexport private keys to an importable file.\r\nTo export an individual key, utilize a key identifier:\r\ngpg --export-secret-keys {{KEY_ID}} > privatekey.key\r\nTo export the trustdb run\r\ngpg --export-ownertrust > ownertrust.txt.\r\nAdd Keys to GitHub\r\nNavigate to https://github.com/settings/keys and add the key to your\r\nGitHub account.\r\nAlternatively, utilize gh-cli to automatically upload\r\nthe GPG key for you via:\r\ngh gpg-key add <key-file>\r\nImporting Keys\r\ngpg --import privatekeys.asc\r\ngpg --import publickeys.asc\r\ngpg --import-ownertrust ownertrust.txt\r\nTesting Keys\r\ngpg -k\r\ngpg -K\r\nAs the new user, test encryption and decryption with\r\ngpg -er <USERID> and gpg -d\r\ncommands.\r\nKeep in mind that decryption and signing will likely fail unless the\r\nuser running gpg owns the terminal it is running on\r\n(Translation: don’t su over to the new user; login directly\r\nvia ssh or console).\r\nScripts\r\nInstallations\r\nLinux:\r\nsudo apt update\r\nsudo apt -y upgrade\r\nsudo apt install socat gpg\r\necho pinentry-program /mnt/c/Program\\ Files\\ \\(x86\\)/Gpg4win/bin/pinentry.exe > ~/.gnupg/gpg-agent.conf\r\ngpg-connect-agent reloadagent /bye\r\nWindows:\r\nsudo cinst -y gpg4win\r\nBackup GPG Keys\r\nSee backup-gpg-keys.sh\r\nfor bash/shell implementation:\r\n#!/usr/bin/env bash\r\n\r\n# backup private keys to asc file\r\ngpg --export-secret-keys --armor \"Jimmy Briggs\" > private-keys-backup.asc\r\n\r\n# backup trustdb\r\ngpg --export-ownertrust > trustdb-backup.txt\r\nSee export-gpg-keys.ps1\r\nfor PowerShell script to Export keys from Windows side:\r\ngpg -a --export > $HOME\\.dotfiles\\gpg\\backup\\public-keys.asc\r\ngpg -a --export-secret-keys > $HOME\\.dotfiles\\gpg\\backup\\private-keys.asc\r\ngpg --export-ownertrust > $HOME\\.dotfiles\\gpg\\backup\\owner-trust.txt\r\nImport Key Scripts\r\nThen, re-import using PowerShell like so:\r\ngpg --import $HOME\\.dotfiles\\gpg\\backup\\private-keys.asc\r\ngpg --import $HOME\\.dotfiles\\gpg\\backup\\public-keys.asc\r\ngpg --import $HOME\\.dotfiles\\gpg\\backup\\owner-trust.txt\r\n\r\n# Test:\r\ngpg -k\r\ngpg -K\r\nTo import using bash:\r\n# restore private keys from asc file\r\ngpg —-import gpg-secret-key-backup.asc\r\n\r\n# delete existing trust database\r\nrm ~/.gnupg/trustdb.gpg\r\n\r\n# restore the trustdb\r\ngpg --import-ownertrust < trustdb-backup.txt\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-07-12T20:27:07-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-08-building-blocks-of-a-comprehensive-actuarial-analyses/",
    "title": "Building the Foundation of a Comprehensive Actuarial Analysis",
    "description": "In Part I of the multi-post series I introduce how to build the foundation of a Comprehensive Actuarial Analysis.",
    "author": [
      {
        "name": "Jimmy Briggs",
        "url": "https://github.com/jimbrig"
      }
    ],
    "date": "2022-03-08",
    "categories": [],
    "contents": "\r\nPart I - Building the Foundation\r\n\r\nThe Building Blocks of a Comprehensive Actuarial Analyses\r\n\r\nThe series The Building Blocks of a Comprehensive Actuarial Analyses kicks off with Part I: Building the Foundation.\r\nThis article will walk through, in detail, the various aspects that a comprehensive actuarial analyses requires to get off the ground and running by building a solid foundation for the rest of the analyses to build from.\r\nNote that this series is meant specifically for non-life, P&C related actuarial analyses and does not pertain to life insurance techniques or actuarial methodologies.\r\nContents\r\nScope and Data Requirements\r\nData Collection\r\nCurrent Loss Run\r\nHistorical Loss Runs\r\nExposure Data\r\nCommon Exposure Metrics\r\n\r\nRetentions, Limits, and Deductibles\r\nALAE Treatment\r\n\r\n\r\nData Collection Conclusions\r\n\r\nScope and Data Requirements\r\nIn order to optimize the return on investment for any actuarial analyses, it is important to begin with some high-level questions to discuss with other key stakeholders for clarification and transparency on what the desired outcome of the project is exactly.\r\nBegin by discussing how the analyses will apply to the unique business operations and plans of the company.\r\nWalk through these important questions in order to build the initial scope of the analyses:\r\nWhat are you trying to accomplish with the actuarial analyses?\r\nWhat is the overall structure of the program, including coverages and policy years included (experience period)?\r\nHave there been any changes in reserving practices, third-party administrators or claim reporting procedures over the years?\r\nWhat operational details of your company/client affect loss experience?\r\nWhat types of loss control programs have been implemented?\r\nAsking these questions and determining accurate answers ensures the clarification of a baseline to build the scope of the desired requirements for the actuarial analyses.\r\nData Collection\r\nAfter answering the preliminary clarification questions, the next step is to begin gathering all necessary data, starting with the most important, required pieces of data:\r\nCurrent Loss Run\r\nFirst and foremost is a current loss run for all open and closed claims. In general, loss runs include the following fields for any individual claim:\r\nClaim Number - Unique identifier for any given claim.\r\nClaimant - The name of the claimant issuing the claim against the insurance provider.\r\nLoss Date - The date on which the accident causing loss or damages occurred.\r\nReport Date - The date on which to insurance provider became aware of the claim.\r\nStatus - The current status of the claim as of the evaluation date of the loss run: Open or Closed\r\nState - geographic information about where the claim occurred with relation to coverages and policies (if segmented by state, for example).\r\nAccident Description - Text describing what happened\r\nIncurred or Reported Losses - Total paid to-date plus outstanding reserves on a claim.\r\nPaid Losses - Total amount paid to-date on a claim.\r\nOutstanding Reserves - Estimated amount left to be paid on a claim.\r\n\r\nLoss runs should be provided for at least the past five policy periods and should include all years with any remaining open claims.\r\nHistorical Loss Runs\r\nAnnually Evaluated Loss Runs for past policy periods are needed in order to compile loss development triangles, which track year-over-year development on incurred loss, paid loss, and reported claim counts.\r\nPast development history is used to estimate future development in losses from the current evaluation date to the ultimate, estimated paid amount when all claims are closed. If past loss runs are not available, industry loss development factors should be utilized instead.\r\nNote that, when the amount of historically evaluated loss runs utilized increases, not only is it easier to identify trends and patterns over time, but the more reliable the loss data becomes.\r\nIn other words, leveraging historical data builds statistical credibility into the actuarial model, allowing the projection of ultimate losses and reserves to be less dependent on industry patterns and more specific to the provided data’s intrinsic trends and characteristics.\r\nCredibility Theory stems directly from the fundamental theory behind the Statistical Laws of Probability and Bayesian Statistics:\r\nThe more data you have, the more stable that data becomes, and therefore the more we can rely on that data to base future outcomes off of. The less data you have, the data will inevitably vary more and therefore more variability should be placed on the predicted future outcomes and external credible sources of data should be used to provide weights on data-driven predictions.\r\nExposure Data\r\nExposure information for at least the past five policy periods and for the projected next period is used to adjust historical losses by the volume of exposure for each policy year. Exposure data should tie to provided loss data with regard to what is received and used in the analysis, meaning that loss amounts aggregated by year and other groupings should have the same unit of measurement as the exposure data in order to derive rates (losses over exposures).\r\nAdditionally, exposure data should tie-out or match up with the aggregated claims data at the policy defined level of granularity. For example if policies are segmented out by coverage, year, line of business, business unit, state, and department, then exposure data should be provided at the same level of granularity where possible. If the analyses requires the allocation of future premiums or reserve estimates to separate entities, this is very important in order to project the correct riskiness associated with each entity being covered. If exposures cannot be segmented to the necessary level of granularity, but the provided claim’s data in the loss runs does, then loss amounts could potentially be used to provide an assumed split-out of the exposures (for example loss weights by state in a ten year experience period can be applied to exposures assuming that they fall in-line with the assumed exposure-driven loss amounts).\r\nPolicy premiums should also directly correlate with exposure trends over time. Policy Premiums can even be used as an exposure metric for deriving earned premiums and estimated loss rates.\r\nExposures may also have non-claim related attributes and dimensions, which in turn have intrinsic consequences on the exposure data’s treatment in the actuarial models. The aggregation of exposure data can include details that indicate separate distinctions of exposure levels and further segmentation may be necessary. For example, separating Fleet Trucks vs. Passenger Vehicles in AL, Full time vs. Part Time Employees in WC, or Neurology Physician vs. Nurse Assistants in MPL.\r\nThese levels should ideally have some form on quantification for comparison with each other in order to allow leveling the exposures out using weighting techniques and deriving separate loss rates representing the level of risk by group.\r\nCommon Exposure Metrics\r\nTypical exposure metrics used by coverage include, but are not limited to:\r\nWorker’s Compensation:\r\nPayroll\r\nNumber of Employees\r\n\r\nGeneral Liability:\r\nRevenue\r\nEarnings\r\n\r\nAutomobile Liability:\r\nNumber of Vehicles\r\nMileage\r\nEstimated price of vehicles\r\n\r\nProperty Liability:\r\nSquare Footage\r\nEstimated Value\r\n\r\nMedical Malpractice or Medical Professional Liability (MPL):\r\nPhysician Headcount\r\nHospital Bed Counts or Occupancy levels\r\n\r\nbut in practice, exposures can come from a widespread variety of sources related to the business.\r\nRetentions, Limits, and Deductibles\r\nRetention levels for each policy period are used to cap losses at per-occurrence retentions, aggregate retentions or quota share amounts declared by the policy. These limited claim amounts are used to derive the actual calculations for determining ultimate losses.\r\nNot only do these limits provide a way to mitigate large claim outliers in order to derive smooth development factors, but they also have direct consequences when deriving ultimate losses based off the development methods.\r\nFor example, if a claim has already breached its $500,000 limit and the company is no longer responsible to any excess amounts greater than $500,000, there is no point in developing ultimate losses for that claim and further and this should be taken into account during loss development.\r\nALAE Treatment\r\nAdditionally, the treatment of Allocated Loss Adjustment Expenses (ALAE) is an important piece of information here. This determines whom is responsible for paying for claim expense amounts and typically falls into one of three categories:\r\nLoss and ALAE - insured is responsible for both loss and ALAE, and both loss and ALAE erode retention limits.\r\nLoss Only - insured is only responsible for loss amounts and only loss amounts erode the retention limits.\r\nPro-Rata - The insured is responsible to a relative portion of ALAE (expenses), relative to the percent of the total claim’s loss amount that is loss only.\r\nFor example, in a Worker’s Compensation, Pro-Rata ALAE Policy scenario, if a claim has the following loss amounts split out between indemnity, medical and expense:\r\nIndividual Claim Limit: $500,000\r\nIndemnity\r\nMedical\r\nExpense\r\nTotal\r\n$320,000\r\n$100,000\r\n$50,000\r\n$470,000\r\nwith Pro-Rata ALAE eroding the retention limit of $500,000 per-claim the amount the insured is responsible for would be:\r\n\\((\\$320,000 + \\$100,000) / \\$470,000 = 89.36\\% * \\$50,000 = \\$44,680.85\\)\r\n$44,680.85 versus the total $50,000 in expenses or ALAE because only 89.36% of the total amount is based off non-expense related values.\r\nData Collection Conclusions\r\nReliable data and transparent discussion with your actuary on the offset of the analysis will help produce more reliable results from the analysis, in turn impacting balance sheet values, and influencing loss budgeting and financial decisions of the company.\r\nIn summary, data collection should include:\r\nClaim level data\r\nExposure level data\r\nPolicy level data\r\nRemember that both current and historical values for these categories of data (especially loss data) should include the current or latest data, historically evaluated data, and when necessary projected future period data.\r\nOne piece of data I left out here is Industry Data. Industry data will be addressed later on and will be considered its own separate process since it is less about collecting it and more about how to utilize it best.\r\nI also did not include Premiums as its own separate data category as it is assumed these would be included in the policy level data provided.\r\nSummary and What’s Next\r\nIn summary, this article provides an intuitive look at how actuaries should frame their projects in order to create the right mindset, determine the scope and requirements of the analyses with stakeholders, and gather the necessary data in order to build the foundation to build the rest of the analyses off of.\r\nDiscuss and determine the business-case for the actuarial analyses through specific questions discussed with key stakeholders related to the project.\r\nDetermine a set of requirements that in turn provide what is and what is not included in the scope of the analyses.\r\nGather and collect necessary data, keeping in mind the value each category of data brings to an analyses’ outcome.\r\nNext up after data collection is beginning the process of deriving the actuarial [[Loss Development Factors]] and summarizing claims data into actuarial triangles.\r\nNext: Part II - Loss Development Factors - The Building Blocks of a Comprehensive Actuarial Analysis.\r\nAppendix: Links\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-07-12T19:44:19-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-04-create-a-custom-r-project-launcher/",
    "title": "Create a Custom R Project Launcher",
    "description": "Introducing a simple, yet powerful tool I created to quickly launch any R project.",
    "author": [
      {
        "name": "Jimmy Briggs",
        "url": "https://github.com/jimbrig"
      }
    ],
    "date": "2022-01-04",
    "categories": [],
    "contents": "\r\nWant to increase your developer productivity? How bout a quick and easy way to list, prioritize, and launch your R Projects from anywhere on your machine, whether RStudio is open or not.\r\nTo start with the end in mind here is a quick demo of the allmighty R Project Launcher I created:\r\n\r\n\r\nI have found the best way to accomplish this (as opposed to the norm) is to identify an area to enhance and create your own personalize solution to that problem. This differs from the typical route of utlizing someone else’s creation or package/library.\r\nI chose to solve\r\nDistill is a publication format for scientific and technical writing, native to the web.\r\nLearn more about using Distill at https://rstudio.github.io/distill.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-07-12T19:44:19-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-18-1-dealing-with-images-in-r/",
    "title": "Dealing with Images in R",
    "description": "A walkthrough of dealing with images in R and creating a custom image gallery.",
    "author": [
      {
        "name": "Jimmy Briggs",
        "url": {}
      }
    ],
    "date": "2021-12-18",
    "categories": [],
    "contents": "\r\nWhile developing and designing software projects in R it is very common to have to deal with various images and the process of manipulating, re-sizing, cropping, and dealing with these images can be a huge pain and waste of time.\r\nExamples of Pain Point with Images\r\nSome examples of the pain points when dealing with images are:\r\nDealing with image formats and file extensions\r\nHaving to remove backgrounds from images to make them transparent\r\nResizing images to fit properly into your webapp\r\nCreating thumbnails and icons from the image for various design layouts\r\netc.\r\nThe magick Package\r\nRecently I finally decided to take a gander into the popular magick R library.\r\nThis package is a swiss-army-knife for dealing with images in R.\r\nA good place to start is the image_resize function which allows you to resize the dimensions of your images. To add to that, let’s create a function allowing one to:\r\nRead in an image file: magick::image_read\r\nResize that image: magick::image_resize\r\nSave the image back to file: magick::image_write\r\n\r\n\r\n\r\nCreating a Gallery of Images\r\n\r\n\r\n\r\nSpice up the Gallery with Custom CSS and Javascript\r\nlightgallery.js\r\nHTML Head needs:\r\n<head>\r\n\r\n<link type=\"text/css\" rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.0/css/lightgallery.min.css\" />\r\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/lightgallery-js/1.4.1-beta.0/js/lightgallery.min.js\"><\/script>\r\n\r\n<!-- lightgallery plugins -->\r\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/lg-fullscreen/1.2.1/lg-fullscreen.min.js\"><\/script>\r\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/lg-thumbnail/1.2.1/lg-thumbnail.min.js\"><\/script>\r\n\r\n<\/head>\r\nPlus the following CSS customization:\r\n#lightgallery > a > img:hover {\r\n   transform: scale(1.15, 1.15);\r\n   transition: 0.4s ease-in-out;\r\n   cursor: pointer;\r\n}\r\nResult\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-07-12T19:44:19-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-18-2-r-package-developer-essentials/",
    "title": "R Package Developer Essentials",
    "description": "A walkthrough of the various packages, tools, resources, and axioms I have \nencountered over my years as an R package sofware engineer.",
    "author": [
      {
        "name": "Jimmy Briggs",
        "url": "https://devfolio.jimbrig.com"
      }
    ],
    "date": "2021-12-18",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nPre-Requisite Readings\r\nPackage Development Fundamentals\r\n\r\nPeople who program using the R language can be separated into two core buckets:\r\nR Developers\r\nR Users\r\nTo distinguish, R developers develop new innovative solutions while R Users use the tools and packages that the developers created. This article is aimed towards the development side of the spectrum and attempts to provide a comprehensive, curated toolbox for the R Developer, specifically in regard to developing R Packages.\r\nPre-Requisite Readings\r\nTo start, R package developers need to be familiar with the fundamentals of package development, and therefore should be familiar with some of the highest regarded resources available to read.\r\nAnyone who is serious about developing production grade R packages needs to be familiar with most of, if not all of the following resources and guides:\r\n\r\n\r\nTable 1: R Package Developer Essential Resources\r\n\r\n\r\nResource\r\n\r\n\r\nSource\r\n\r\n\r\nWriting R Extensions\r\n\r\n\r\nCRAN Manuals\r\n\r\n\r\nR Packages\r\n\r\n\r\nHadley Wickham\r\n\r\n\r\nR Package Primer\r\n\r\n\r\nKarl Broman\r\n\r\n\r\nPackage Guidelines\r\n\r\n\r\nBioconductor\r\n\r\n\r\nrOpenSci Packages Developer Guide\r\n\r\n\r\nrOpenSci\r\n\r\n\r\n\r\nView Citations\r\n\r\n(Wickham and Bryan, n.d.a, n.d.b; “Why Package and Environment Management Is Critical for Serious Data Science,” n.d.a, n.d.b; Vidoni, n.d.; Initiative, n.d.; “An Introduction to Packager,” n.d.; Gandrud 2015; Glennie 2020; “Owen-TheRGuide.pdf,” n.d.; Riederer, n.d.; Spector 2004; Team, n.d.; Zhu and Jianan, n.d.)\r\n\r\nWriting R Extensions Manual\r\nThe Writing R Extensions Manual is perhaps the most crucial resource listed above, and has even been considered the Bible of R Package Development.\r\nHowever, it is very exhaustive and not the most visually-appealing write-up.\r\nAs Hadley puts it in his book Writing R Packages: (Wickham and Bryan, n.d.c)\r\n\r\n“The best resource for the official details of package development is always the official writing R extensions manual. However, this manual can be hard to understand if you’re not already familiar with the basics of packages. It’s also exhaustive, covering every possible package component, rather than focusing on the most common and useful components, as this book does. Writing R extensions is a useful resource once you’ve mastered the basics and want to learn what’s going on under the hood.”\r\n\r\n— Hadley Wickham\r\n\r\nThanks to Colin Fay, a more elegant version of the original manual has been created as a bookdown site and published online at https://colinfay.me/writing-r-extensions.\r\nThis resource is highly encouraged for anyone taking R Package Development seriously.\r\nNote: The other manuals listed on the CRAN Manuals website contain a lot of hidden gems that are often overlooked by R developers. These resources contain some of the most crucial, foundational knowledge that anyone using R should eventually be made aware of, therefore I highly recommend you check those out in addition to Writing R Extensions.\r\nPackage Development Fundamentals\r\nR Package development can be broken down into the following fundamental areas of development:\r\n\r\n\r\n\r\n“An Introduction to Packager.” n.d. https://cran.r-project.org/web/packages/packager/vignettes/An_Introduction_to_packager.html.\r\n\r\n\r\nGandrud, Christopher. 2015. Reproducible Research with r and RStudio. Second edition. Chapman & Hall/CRC the r Series. Boca Raton: CRC Press, Taylor & Francis Group.\r\n\r\n\r\nGlennie, Richard. 2020. “Reproducible Research with r and RStudio by Christopher Gandrud.” Journal of Agricultural, Biological and Environmental Statistics, 12.\r\n\r\n\r\nInitiative, Environmental Data. n.d. Best Practices for Data Package Design. https://ediorg.github.io/data-package-best-practices/datapackage-design/index.html.\r\n\r\n\r\n“Owen-TheRGuide.pdf.” n.d. https://cran.r-project.org/doc/contrib/Owen-TheRGuide.pdf.\r\n\r\n\r\nRiederer, Yihui Xie, Christophe Dervieux, Emily. n.d. R Markdown Cookbook. https://bookdown.org/yihui/rmarkdown-cookbook/.\r\n\r\n\r\nSpector, Phil. 2004. “An Introduction to r.” University of California, Berkeley. https://www.stat.berkeley.edu/~spector/R.pdf.\r\n\r\n\r\nTeam, R. Core. n.d. Writing r Extensions. https://colinfay.me/writing-r-extensions/index.html.\r\n\r\n\r\nVidoni, rOpenSci software review editorial team (current and alumni): Brooke Anderson, Scott Chamberlain, Laura DeCicco, Julia Gustavsen, Jeff Hollister, Anna Krystalli, Mauro Lepore, Lincoln Mullen, Karthik Ram, Emily Riederer, Noam Ross, Maëlle Salmon, Adam Sparks, Melina. n.d. rOpenSci Packages: Development, Maintenance, and Peer Review. https://devguide.ropensci.org/.\r\n\r\n\r\n“Why Package and Environment Management Is Critical for Serious Data Science.” n.d.a. https://blog.rstudio.com/2020/08/20/why-package-environment-management-is-critical-for-serious-data-science/.\r\n\r\n\r\n———. n.d.b. https://blog.rstudio.com/2020/08/20/why-package-environment-management-is-critical-for-serious-data-science/.\r\n\r\n\r\nWickham, Hadley, and Jennifer Bryan. n.d.c. R Packages. https://r-pkgs.org/.\r\n\r\n\r\n———. n.d.a. R Packages. https://r-pkgs.org/.\r\n\r\n\r\n———. n.d.b. R Packages. https://r-pkgs.org/.\r\n\r\n\r\nZhu, Yang Feng, and Jianan. n.d. R Programming: Zero to Pro. https://r02pro.github.io/.\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-07-12T19:44:19-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-11-experimenting-with-caching-backends-in-r/",
    "title": "Experimenting with Caching Backends in R",
    "description": "Take a look at some of the more advanced, modern ways of utilizing backend caching with hosted applications or workflows.",
    "author": [
      {
        "name": "Jimmy Briggs",
        "url": "https://github.com/jimbrig"
      }
    ],
    "date": "2021-12-11",
    "categories": [],
    "contents": "\r\nIntroduction\r\nThe R ecosystem provides an assortment of options and libraries for implementing caching into one’s data-related workflows, for example:\r\nmemoise\r\ncachem\r\nRcache\r\ncacher\r\nstorr\r\nplus many others.\r\nHowever, there seems to be a slight lack in support for caching with regard to cloud hosted workflows and web applications.\r\nTypical Web Application’s Architecture\r\nAs an example, take a typical web-application architecture where the process is segmented into three separate layers:\r\n This generalized architecture is deemed the Three-Tier Architecture and is a model generally used by app developers to create flexible and reusable applications.\r\nBroken down, the three layers are:\r\nPresentation Layer - the interface the end-user interacts with. Its primary duty is to translate tasks and results from servers into output that the end-user can understand. In short, this tier is the User Interface or client.\r\nApplication Layer - primary coordinator of the back-end of the application. This tier processes commands, makes logical decisions and processes data between the end user-interface and the back-end database. In short, this tier is the business logic.\r\nData Layer - simply put, this tier is where data is stored and accessed by the application layer. This tier composes of the various data stores needed by the application to access and retrieve data used in both previously mentioned layers, i.e. databases and cloud storage.\r\nA benefit of the three-tier architecture is that each layer can work and be maintained, developed, and tested in isolation, independent of the other layers. Therefore every time the application queries for data, the speed is limited by network performance.\r\nData retrieval time plays a key role in overall user experience (UX) and is a critical requirement for most large applications meant for production.\r\nDatabase Caching Overview\r\nCaching is a buffering technique that stores frequently-queried data in a temporary memory. It makes data easier to be accessed and reduces workloads for databases. For example, you need to retrieve a user’s profile from the database and you need to go from a server to server. After the first time, the user profile is stored next (or much nearer) to you. Therefore, it greatly reduces the time to read the profile when you need it again.\r\nThe cache can be set up in different tiers or on its own, depending on the use case. It works with any type of database including both Relational and NoSQL Databases.\r\nBenefits of Caching\r\nPerformance — Performance is improved by making data easier to be accessed through the cache and reduces workloads for database.\r\nScalability — Workload of back-end query is distributed to the cache system which is lower costs and allow more flexibility in processing of data.\r\nAvailability — If back-end database server is unavailable, cache can still provide continuous service to the application, making the system more resilient to failures.\r\nOverall, it is the minimally invasive strategy to improve application performance by implementing caching with additional benefits of scalability and availability.\r\nThere are many implementations of caching in modern application frameworks including:\r\nCache Aside\r\nRead Through\r\nWrite Through\r\nWrite Back\r\nWrite Around\r\nFor example here is an architectural diagram of the Cache Aside workflow:\r\n\r\nOk so now moving onto caching in relation to R:\r\nCaching in Shiny\r\nRecently, the shiny package introduced new functions that aid in caching: bindCache and renderCachedPlot.\r\nrenderCachedPlot() requires shiny version 1.5.0 or higher.\r\nbindCache() requires shiny version 1.6.0 or higher.\r\nAdding a Cache Layer Between Application and Database\r\nRedis is the most commonly used back-end for implementing a caching layer between the application and database in production, and it is pretty awesome.\r\nOne can easily spin up a local Redis backend using docker as follows:\r\ndocker run --rm --name redisbank -d -p 6379:6379 redis:5.0.5 --requirepass bebopalula\r\nThis will run a redis container at localhost exposed to port 6379 (Redis’ typical port).\r\nNext, you can add a redis cache to your R workflow using a generated R6 object that represents the redis_cache created with the bank package in conjunction with the memoise package as so:\r\nNote: the redux package will be required to implement the redis_cache R6 object here\r\nlibrary(bank)\r\nlibrary(memoise)\r\n\r\nredis_cache <- bank::cache_redis$new(password = \"bebopalula\")\r\n\r\nf <- function(x) {\r\n  sample(1:1000, x)\r\n}\r\n\r\nmf <- memoise::memoise(f, cache = redis_cache)\r\n\r\nmf(5)\r\n\r\nmf(10)\r\nand inside Shiny:\r\nui <- fluidPage(\r\n  # Creating a slider input that will be used as a cache key\r\n  sliderInput(\"nrow\", \"NROW\", 1, 32, 32),\r\n  # Plotting a piece of mtcars\r\n  plotOutput(\"plot\")\r\n)\r\n\r\nserver <- function(input, output, session) {\r\n  output$plot <- renderCachedPlot(\r\n    {\r\n      # Pretending this takes a long time\r\n      Sys.sleep(2)\r\n      plot(mtcars[1:input$nrow, ])\r\n    },\r\n    cacheKeyExpr = list(\r\n      # Defining the cache key\r\n      input$nrow\r\n    ),\r\n    # Using our redis cache\r\n    cache = redis_cache\r\n  )\r\n}\r\nshinyApp(ui, server)\r\nFor a more involved shiny example you could use:\r\ngenerate_app(redis_cache)\r\nTry it Out!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-11-experimenting-with-caching-backends-in-r/images/caching.jpg",
    "last_modified": "2022-07-12T19:44:19-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-07-searching-from-r/",
    "title": "Searching from R",
    "description": "'Utilize advanced searching techniques to quickly launch a search from R'",
    "author": [
      {
        "name": "Jimmy Briggs",
        "url": "https://github.com/jimbrig"
      }
    ],
    "date": "2020-12-07",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nYour Mind is a CPU\r\n\r\nSearching from R\r\nIdentifying an Opportunity\r\nReviewing Current Processes\r\nDeveloping a Solution\r\nSetting up the Search Engines\r\nExamples\r\n\r\nR Code\r\nGeneral Search Engine\r\nSearch Github\r\nRseek.org\r\nSearch R Site via Finzi\r\nSearch Nabble R Forum\r\nSearch Google\r\nSearch R Project Domain via Google\r\nSearch METACRAN\r\n\r\nConcluding Remarks\r\n\r\n\r\n\r\nIntroduction\r\nSkip to the R code examples.\r\nProgrammers are good at engineering software and designing web applications. I argue that we can apply the same logic to facilitate an environment which provides a natural, orchestrated flow between the various activities we undertake, both online and in the console.\r\nIn this post I use searching the web, a process developers rely on heavily for many different reasons, as a basic example of an area for improvement.\r\nI will show you some examples of querying specific search engines directly from your R console as well as provide some pre-configured functions for some of my most common use-cases.\r\nBefore showcasing the code, first I want to bring to light some ideas surrounding our daily workflows as developers and how we interact with the web.\r\nYour Mind is a CPU\r\n\r\n“The optimal number of threads in any system is one thread.” - Scott Hanselman\r\n\r\nThink of your mind as a CPU (central processing unit) like that on your computer. Each separate task you pursue throughout the day relies on a limited amount of resources, which can be thought of as your bank of availableRandom Access Memory, or RAM.\r\nFrom this perspective, the idea of multi-tasking becomes multi-threading, and as developers we all know that for computers to run asynchronous, parallel, multi-threaded processes requires a highly intricate, well designed, and deeply thought out infrastructure to work properly without overloading the CPU and crashing the machine.\r\nSimilarly, switching contexts between different digital environments causes your mind to have to continuously re-adjust and reboot its underlying resources in order to facilitate the influx of new information in need of being processed.\r\nIn other words, it takes time, energy, and resources to get your mind to focus, and every time we switch tasks or contexts, we lose energy that wouldn’t have been lost if we had just stayed in one context initially. The fatigue that builds up from all of this energy loss can potentially demotivate us, as well as cause mental burnout.\r\nProgrammers should strive to reach a state of flow in which complete focus can be given to one activity.\r\nThis requires determination and discipline in order to keep that focus streamlined as well as a proper environment which motivates focus instead of scattered, distracted thought.\r\nThe solution to avoiding burnout and unproductive programming habits is to provide an ecosystem where context switching is minimized and when needed, streamlined.\r\nSearching from R\r\nIdentifying an Opportunity\r\nHave you ever wanted to search google directly from R? What about query your organizations Github repos or search for packages on your respective CRAN mirror?\r\nThis thought never crossed my mind until I took a second to think about the potential benefits of constructing a system for implementing this in R.\r\nThe problem I found was two-fold:\r\nSearching the web for R specific resources can be frustrating, time-consuming, and difficult.\r\nThe process of switching contexts from R to the browser is unproductive and breaks my natural development flow.\r\nIdentifying these issues, I took it as an opportunity to create something new, and potentially beneficial to others as we developers love to do.\r\nReviewing Current Processes\r\nAlthough most R developers are extensively familiar with The DRY Principle (Don’t repeat yourself) in programming, many do not attempt to extend the principle outside of their code.\r\nThinking on the principle further, I realized that the amount of time developers interact with the web through a web browser interface is much larger than we may think and is only growing as our dependence on online services and learning resources increase exponentially.\r\nFor me, the idea of searching from your command prompt or terminal stemmed from my repetitive need to search online for some form of information whilst in the midst of a programming endeavor. While there are many amazing API’s, SDK’s, and localized web-oriented frameworks allowing developers to interact with the web from their console, I found myself in need of a more general, simplistic approach to querying information hosted online from my local machine.\r\nOnce I stopped to think about my daily processes while developing locally and interacting with the internet, I quickly realized that there is a much larger frequency of instances where I am in need of browsing the web than I originally would have anticipated. Whether navigating to my web browser to google a new concept, review a Github pull request, or even launch a locally hosted shiny application, I found that I use my browsers constantly throughout the day.\r\nTo name a few examples:\r\nReviewing a new pull request on Github or searching my backlog of issues assigned to me\r\nSearching for new software or downloading a new version\r\nSearching my cloud storage drives for files or data\r\nSearching my emails for important information\r\nReviewing online documentation when learning something new\r\nRetrieving a forgotten API Key or account password\r\nChecking on and maintaining a hosted cloud service platform\r\nWatching a demo video on youtube for a new application\r\nChecking my RSS feeds and developer blogs to stay up to date on current news\r\nSearching google for a new library, concept, or framework\r\nAs you can see, there are a variety of instances where interacting with and querying the web has become a routine in our daily development workflows and learning processes.\r\nAdditionally, due to the lack of a more streamlined, self-contained approach to this reliance on the web while programming, I found myself easily distracted my all of the bells and whistles the internet and my browsers have to offer. Between all of the bookmarks, feeds, notifications and variety of browser extensions the simple process of leaving your code and opening your browser has much larger implications than most people realize.\r\nWhile this may seem like a minimal interruption to your programming workflow, the fact of the matter is that programming is a form of work which requires a very deep, specific environment and mindset in order to keep your development pipelines productive and efficient. In other words, programming is not easy and requires intense focus.\r\nTherefore, developers should do everything they can to avoid having to switch in and out of the various shallow contexts surrounding our everyday lives while attempting to perform a session of productive coding.\r\nDeveloping a Solution\r\nBrowser Search Engines\r\nTo provide a simple, yet powerful solution to the issues surrounding browsing the internet for necessary resources, I started with my personal browser settings.\r\nMost web browser providers facilitate a setting where you can configure your own custom configurations for creating search queries to specific web sites using your address bar.\r\nFor example, in Chrome if you navigate to Settings > Manage Search Engines you will see a list of default search engine providers such as google, bing, and yahoo as well as some auto-generated engines created by websites you have visited in the past such as Youtube or Medium.\r\nWhat most people do not realize is they can add add their own search engines and keywords also.\r\nFor example, if I want to search Github repositories querying with a specific keyword while filtering for only repositories categorized with the language R the query URL would be: https://github.com/search?q=%slanguage%3Ar.\r\nYou can produce this yourself by searching from Github and applying the filters yourself and replacing the term you searched for with %s, similar to the sprintf context. Lastly, assign a keyword of ghr and test it out by typing ghr in your address bar and pressing tab or enter. Now when you search it will direct you to Github R repositories!\r\nExample: Search Github for tidyverse with keyword ghr filtering for Language:R:\r\n\r\nnote that this is setup with the following in chrome://settings/searchEngines:\r\n\r\nBuilding on this framework I setup in my browser years ago, I decided to take it one step further by bringing the searching functionality to my personal R package browsr focusing on R-specific search engines (it can be quite difficult to search effectively for R resources filtering out everything else given R is only one letter).\r\nSetting up the Search Engines\r\nFor setting up the search engines all I needed to do was migrate my browser customized search engine URL’s into R by wrapping them in R functions which pass the %s syntax as a function argument.\r\nHere is a list of the search engines I have implemented so far and their functions:\r\nGeneral Search Function\r\nSearch Github\r\nSearch RSeek.org\r\nSearch Finzi’s R Site\r\nSearch the R Forum on Nabble\r\nSearch Google\r\nSearch R-Project Domain from Google\r\nSearch METACRAN\r\nExamples\r\nHere is a list of some common examples:\r\nGmail: https://mail.google.com/mail/ca/u/0/#apps/%s\r\nYouTube: https://www.youtube.com/results?search_query=%s&page={startPage?}&utm_source=opensearch\r\nFacebook: https://www.facebook.com/search/top/?q=%s&opensearch=1\r\nTwitter: https://twitter.com/search?q=%s\r\nSoundCloud: https://soundcloud.com/search?q=%s\r\nAmazon: https://www.amazon.in/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=%s\r\nSome may find it useful to create custom github queries for their Github accounts:\r\nYour personal Github repositories: https://github.com/<GITHUB_USERNAME>?tab=repositories&q=%s\r\nYour organizations Github repositories: https://github.com/search?q=org%3A<GITHUB_ORGANIZATIoN>+%s\r\nExample Tables\r\nGeneral\r\nSearch Engine\r\nKeyword\r\nQuery URL\r\nEdge Settings\r\nsettings\r\nedge://settings/?search=%s\r\nEdge History\r\nhist\r\nedge://history/all?q=%s\r\nGmail Inbox Search\r\ngmail\r\nhttps://inbox.google.com/search/%s\r\nGoogle Drive\r\ngdrive\r\nhttps://drive.google.com/drive/u/0/search?q=%s\r\nGithub\r\ngh\r\nhttps://github.com/search?q=%s&ref=opensearch\r\nGithub Gist\r\ngist\r\nhttps://gist.github.com/search?q=&ref=opensearch\r\nStackOverFlow\r\nstackoverflow\r\nhttps://stackoverflow.com/search?q=%s\r\nDev.to\r\ndevto\r\nhttps://dev.to/search?q=%s\r\nDevDocs\r\ndevdocs\r\nhttps://devdocs.io/#q=%s\r\nDeveloper Resources\r\nAnd some more general developer resources:\r\nSearch Engine\r\nKeyword\r\nQuery URL\r\nGithub\r\ngh\r\nhttps://github.com/search?q=%s&ref=opensearch\r\nGithub Gist\r\ngist\r\nhttps://gist.github.com/search?q=&ref=opensearch\r\nStackOverFlow\r\nstackoverflow\r\nhttps://stackoverflow.com/search?q=%s\r\nDev.to\r\ndevto\r\nhttps://dev.to/search?q=%s\r\nDevDocs\r\ndevdocs\r\nhttps://devdocs.io/#q=%s\r\nMozilla WebDocs\r\nmozdocs\r\nhttps://developer.mozilla.org/en-US/search?q=%s&w=3&qs=plugin\r\nR-Resources\r\nThe R-Project Website provides some useful search engines here:\r\n\r\n\r\nTaking these references as a starting point we can create some query URLs for searching R specifically:\r\nSearch Engine\r\nKeyword\r\nQuery URL\r\nR Site Search by Finzi\r\nrsite\r\nhttp://finzi.psych.upenn.edu/cgi-bin/namazu.cgi?query=%s&max=100&result=normal&sort=score&idxname=functions&idxname=views\r\nRSeek\r\nrseek\r\nhttps://www.rseek.org/?q=%s\r\nR Nabble Forum\r\nrnabble\r\nhttps://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=search_page&node=789695&query=%s\r\nAdvanced Google Search: R Project\r\nrproj\r\nhttps://www.google.com/search?q=%s&domains=r-project.org&sitesearch=r-project.org&btnG=Google+Search\r\nAdditional R\r\nAdditionally, I have created these R related search queries as well:\r\nSearch Engine\r\nKeyword\r\nQuery URL\r\nGithub - Language:R\r\nghr\r\nhttps://github.com/search?q=%slanguage%3Ar\r\nGithub Gist - Language:R\r\ngistr\r\nhttps://gist.github.com/search?q=%s+language%3Ar&ref=searchresults\r\nStackOverFlow - Tag:R\r\nstackoverflowr\r\nhttps://stackoverflow.com/search?q=%5Br%5D+%s\r\nMETACRAN\r\nmetacran\r\nhttps://www.r-pkg.org/search.html?q=%s\r\nDan Goldstein’s Site\r\ndanr\r\nhttp://www.dangoldstein.com/search_r.html?cx=partner-pub-8815643643661420%3Al9jc9v-e2vi&cof=FORID%3A10&ie=ISO-8859-1&q=%s&sa=Search&siteurl=www.dangoldstein.com%2Fsearch_r.html&ref=search.r-project.org%2F&ss=265j35153j3\r\nR Documentation\r\nrdocs\r\nhttp://rdocumentation.org/\r\nR Package Documentation - Packages\r\nrdrrpkg\r\nhttps://rdrr.io/find/?repos=cran%2Cbioc%2Crforge%2Cgithub&fuzzy_slug=%s\r\nRStudio Website\r\nrstudio\r\nhttps://rstudio.com/#stq=%s&stp=1\r\nR-Bloggers\r\nrbloggers\r\nhttps://cse.google.com/cse?cx=005359090438081006639%3Apaz69t-s8ua&ie=UTF-8&q=%s&sa=Go\r\nR Code\r\nOnline Reference: https://jimbrig.github.io/browsr/reference/search.html\r\nGeneral Search Engine\r\n\r\n\r\n#' Generalized Search and Search Engine Examples\r\n#'\r\n#' Use these functions to search the web directly from your browser,\r\n#' building advanced queries and supplying common useful R related domains.\r\n#'\r\n#' @param s string to search for (`%s` in the query URL)\r\n#' @param query_url string representing the URL to query; defaults to Google\r\n#'\r\n#' @name search\r\n#'\r\n#' @keywords search_engines\r\n#'\r\n#' @export\r\n#'\r\n#' @examples\r\n#' library(browsr)\r\n#' # default search on google\r\n#' search_online(\"polished.tech\")\r\n#'\r\n#' # search github (note: &ref=opensearch)\r\n#' search_online(\"polished\", \"https://github.com/search?q=%s&ref=opensearch\")\r\n#'\r\n#' # search Github with language:r, org:tychobra for 'polished' (note: '%3A' represents a ':')\r\n#' search_online(\"polished\", \"https://github.com/search?q=%s+language%3Ar+org%3Atychobra\")\r\nsearch_online <- function(s,\r\n                          query_url = \"https://google.com/search?q=\") {\r\n\r\n  url <- paste0(query_url, s)\r\n\r\n  utils::browseURL(url)\r\n\r\n}\r\n\r\n\r\n\r\nSearch Github\r\n\r\n\r\n#' Search Github\r\n#'\r\n#' Query Github's internal search engine.\r\n#'\r\n#' @param s string to search for\r\n#' @param type what to search for, see details for options\r\n#' @param language optional language filter\r\n#' @param topic optional topic filter\r\n#' @param user optional user filter\r\n#' @param org optional org filter\r\n#'\r\n#' @describeIn search\r\n#'\r\n#' @export\r\n#'\r\n#' @examples\r\n#' search_gh(\"websocket\", language = \"r\", topic = \"rshiny\")\r\n#'\r\n#' # search your org\r\n#' search_gh(\"polished\", org = \"tychobra\")\r\nsearch_gh <- function(s,\r\n                      type = \"all\",\r\n                      language = NULL,\r\n                      topic = NULL,\r\n                      user = NULL,\r\n                      org = NULL) {\r\n\r\n  types <- c(\"all\",\r\n             \"repo\",\r\n             \"code\",\r\n             \"commit\",\r\n             \"issue\",\r\n             \"discussion\",\r\n             \"package\",\r\n             \"marketplace\",\r\n             \"topic\",\r\n             \"wiki\",\r\n             \"user\")\r\n\r\n  match.arg(type, types)\r\n  type_query <- ifelse(type == \"all\", \"&ref=opensearch\", paste0(\"&type=\", type))\r\n  base_url <- \"https://github.com/search?q=\"\r\n  lang_query <- ifelse(is.null(language), \"\", paste0(\"+language%3A\", language))\r\n  topic_query <- ifelse(is.null(topic), \"\", paste0(\"+topic%3A\", topic))\r\n  user_query <- ifelse(is.null(user), \"\", paste0(\"+user%3A\", user))\r\n  org_query <- ifelse(is.null(org), \"\", paste0(\"org%3A\", org))\r\n  query <- paste0(s, \" \", lang_query, topic_query, user_query, org_query, type_query)\r\n\r\n  url <- paste0(base_url, query)\r\n\r\n  utils::browseURL(url)\r\n\r\n}\r\n\r\n\r\n\r\nRseek.org\r\n\r\n\r\n#' Search RSeek.org\r\n#'\r\n#' Query a search on [rseek.org](https://rseek.org/).\r\n#'\r\n#' @param s string to search for\r\n#' @export\r\n#'\r\n#' @describeIn search\r\n#'\r\n#' @references\r\n#' - <http://www.sashagoodman.com/>\r\nsearch_rseek <- function(s) {\r\n  url <- paste0(\"http://www.rseek.org/?q=\", s)\r\n  utils::browseURL(url)\r\n}\r\n\r\n\r\n\r\nSearch R Site via Finzi\r\n\r\n\r\n#' Search Finzi\r\n#'\r\n#' @param s string to search for\r\n#'\r\n#' @describeIn search\r\n#'\r\n#' @references\r\n#' - <http://finzi.psych.upenn.edu/search.html>\r\n#' - <http://finzi.psych.upenn.edu/search/manual.html#query>\r\n#'\r\n#' @export\r\nsearch_finzi <- function(s) {\r\n\r\n  url <- paste0(\r\n    \"http://finzi.psych.upenn.edu/cgi-bin/namazu.cgi?query=\",\r\n    s,\r\n    \"&max=100&result=normal&sort=score&idxname=functions&idxname=views\"\r\n  )\r\n\r\n}\r\n\r\n\r\n\r\nSearch Nabble R Forum\r\n\r\n\r\n#' Search Nabble R Forum\r\n#'\r\n#' Query a search on the R Nabble Forum. Nabble is an innovative search engine\r\n#' for R messages.\r\n#'\r\n#' @describeIn search\r\n#'\r\n#' @param s string to search for\r\n#'\r\n#' @export\r\n#' @references\r\n#' - <https://cloud.r-project.org/search.html>\r\n#' - <http://n4.nabble.com/help/Answer.jtp?id=31>\r\nsearch_nabble <- function(s) {\r\n\r\n  url <- paste0(\r\n    \"https://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=search_page&node=789695&query=\", s\r\n  )\r\n\r\n  utils::browseURL(url)\r\n\r\n}\r\n\r\n\r\n\r\nSearch Google\r\n\r\n\r\n#' Search Google\r\n#'\r\n#' @describeIn search\r\n#'\r\n#' @param s string to search for\r\n#' @export\r\nsearch_google <- function(s) {\r\n  url <- paste0(\"https://www.google.com/search?q=\", s)\r\n  utils::browseURL(url)\r\n}\r\n\r\n\r\n\r\nSearch R Project Domain via Google\r\n\r\n\r\n#' Search R Project Domain on Google\r\n#'\r\n#' @param s string to search for\r\n#'\r\n#' @describeIn search\r\n#'\r\n#' @export\r\n#'\r\n#' @details See <https://cloud.r-project.org/search.html> which showcases Google's\r\n#'   advanced search feature to query only R-Project domain sites via the\r\n#'   [Google Search Engine](http://www.google.com/advanced_search).\r\nsearch_rproject <- function(s) {\r\n\r\n  url <- paste0(\r\n    \"https://www.google.com/search?q=\", s,\r\n    \"&domains=r-project.org&sitesearch=r-project.org&btnG=Google+Search\"\r\n  )\r\n\r\n  utils::browseURL(url)\r\n\r\n}\r\n\r\n\r\n\r\nSearch METACRAN\r\n\r\n\r\n#' Search METACRAN\r\n#'\r\n#' @param s string to search for\r\n#'\r\n#' @describeIn search\r\n#'\r\n#' @export\r\nsearch_metacran <- function(s) {\r\n\r\n  url <- paste0(\r\n    \"https://www.r-pkg.org/search.html?q=\", s)\r\n\r\n  utils::browseURL(url)\r\n\r\n}\r\n\r\n\r\n\r\nConcluding Remarks\r\nDownload the source code from the Github Repository.\r\nAlthough I do use other terminals, languages, and shells outside of the R environment, most of my development work is done within R and RStudio. Therefore, I chose R as my framework for implementing a set of utility functions to quickly perform advanced search queries online from a wide variety of search engine source domains.\r\nSimilar R Packages:\r\nsearcher\r\nwebsearcher\r\npackageFinder\r\ntools::CRAN_package_db() from the tools package\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-12-07-searching-from-r/images/magnifying_glass.jpg",
    "last_modified": "2022-07-12T19:44:19-04:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to Jim's Docs",
    "description": "Welcome to our new blog, Jim's Docs. We hope you enjoy \nreading what we have to say!",
    "author": [
      {
        "name": "Nora Jones",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2020-12-07",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-07-12T19:44:19-04:00",
    "input_file": {}
  }
]
